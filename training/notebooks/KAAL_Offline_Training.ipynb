{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è KAAL Offline RL Training Notebook\n",
    "\n",
    "## RAKSHAK - Agentic AI Cyber Guardian\n",
    "\n",
    "This notebook trains the **KAAL (Knowledge-Augmented Autonomous Learner)** Dueling DQN agent for autonomous cyber defense.\n",
    "\n",
    "### What is KAAL?\n",
    "KAAL is a reinforcement learning agent that decides defensive actions against cyber threats:\n",
    "- **MONITOR** - Continue observing the threat\n",
    "- **DEPLOY_HONEYPOT** - Deploy a decoy to gather intelligence\n",
    "- **ISOLATE_DEVICE** - Quarantine the compromised device\n",
    "- **ENGAGE_ATTACKER** - Redirect attacker to honeypot\n",
    "- **ALERT_USER** - Send notification to user\n",
    "\n",
    "### Training Approach\n",
    "- **Offline RL**: Train from stored attack events (no live environment needed)\n",
    "- **Dueling DQN**: Separates value and advantage for better learning\n",
    "- **Experience Replay**: Random sampling for stable training\n",
    "\n",
    "### Output\n",
    "- `kaal_policy.pth` - Trained model for Jetson deployment\n",
    "\n",
    "---\n",
    "**Author**: Team RAKSHAK  \n",
    "**Runtime**: GPU recommended (T4 or better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy matplotlib tqdm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving model)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/RAKSHAK_Models'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Dueling DQN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling Deep Q-Network architecture.\n",
    "    \n",
    "    Separates the network into value and advantage streams:\n",
    "    Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\n",
    "    \n",
    "    This helps the agent learn which states are valuable\n",
    "    without having to learn the effect of each action.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int = 10, action_size: int = 5, hidden_size: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Shared feature extraction\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Value stream - estimates V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream - estimates A(s, a)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, action_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.feature(x)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        \n",
    "        # Combine: Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    \"\"\"RL transition tuple.\"\"\"\n",
    "    state: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    done: bool\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, transition: Transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size: int) -> List[Transition]:\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Test the architecture\n",
    "model = DuelingDQN().to(device)\n",
    "test_input = torch.randn(1, 10).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"‚úÖ DuelingDQN created successfully!\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "print(f\"   Output shape: {test_output.shape}\")\n",
    "print(f\"   Q-values: {test_output.detach().cpu().numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Training Data\n",
    "\n",
    "Choose one of the options below:\n",
    "- **Option A**: Generate synthetic training data (for testing)\n",
    "- **Option B**: Upload real events from Jetson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# OPTION A: Generate Synthetic Training Data\n# =============================================================================\n# FIXED: Using EXCLUSIVE optimal action mapping with strong reward signals\n\ndef generate_synthetic_events(num_events: int = 5000) -> List[Dict]:\n    \"\"\"\n    Generate synthetic attack events with EXCLUSIVE optimal actions.\n    \n    Key fix: Each (attack_type, severity) pair maps to exactly ONE optimal action.\n    This prevents the model from learning ambiguous preferences.\n    \"\"\"\n    # Attack types - each has specific optimal action mappings\n    ATTACK_TYPES = ['port_scan', 'brute_force', 'dos_attack', 'malware', \n                    'exploit_attempt', 'data_exfiltration', 'unauthorized_access',\n                    'ping_sweep', 'unknown_device']\n    SEVERITIES = ['low', 'medium', 'high', 'critical']\n    ACTIONS = ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']\n    \n    # ==========================================================================\n    # EXCLUSIVE OPTIMAL ACTION MAPPING\n    # ==========================================================================\n    # Each (attack_type, severity) -> exactly ONE optimal action\n    # This creates clear, unambiguous training signals\n    \n    EXCLUSIVE_OPTIMAL = {\n        # MONITOR scenarios (low severity reconnaissance only)\n        ('port_scan', 'low'): 'MONITOR',\n        ('ping_sweep', 'low'): 'MONITOR',\n        \n        # DEPLOY_HONEYPOT scenarios (brute force - credential capture)\n        ('brute_force', 'low'): 'DEPLOY_HONEYPOT',\n        ('brute_force', 'medium'): 'DEPLOY_HONEYPOT',\n        \n        # ISOLATE_DEVICE scenarios (critical/high severity threats)\n        ('dos_attack', 'critical'): 'ISOLATE_DEVICE',\n        ('dos_attack', 'high'): 'ISOLATE_DEVICE',\n        ('malware', 'critical'): 'ISOLATE_DEVICE',\n        ('malware', 'high'): 'ISOLATE_DEVICE',\n        ('exploit_attempt', 'critical'): 'ISOLATE_DEVICE',\n        ('exploit_attempt', 'high'): 'ISOLATE_DEVICE',\n        \n        # ENGAGE_ATTACKER scenarios (medium severity - intel gathering)\n        ('dos_attack', 'medium'): 'ENGAGE_ATTACKER',\n        ('exploit_attempt', 'medium'): 'ENGAGE_ATTACKER',\n        ('malware', 'medium'): 'ENGAGE_ATTACKER',\n        \n        # ALERT_USER scenarios (user-relevant events)\n        ('data_exfiltration', 'low'): 'ALERT_USER',\n        ('data_exfiltration', 'medium'): 'ALERT_USER',\n        ('unknown_device', 'low'): 'ALERT_USER',\n        ('unknown_device', 'medium'): 'ALERT_USER',\n        ('unauthorized_access', 'low'): 'ALERT_USER',\n        ('unauthorized_access', 'medium'): 'ALERT_USER',\n    }\n    \n    # Only use attack_type + severity combinations that have defined optimal actions\n    VALID_COMBINATIONS = list(EXCLUSIVE_OPTIMAL.keys())\n    \n    events = []\n    base_time = datetime.now()\n    \n    for i in range(num_events):\n        # Pick a valid combination\n        attack_type, severity = random.choice(VALID_COMBINATIONS)\n        optimal_action = EXCLUSIVE_OPTIMAL[(attack_type, severity)]\n        \n        # Severity and attack encoding\n        severity_val = SEVERITIES.index(severity) / 3.0\n        attack_val = ATTACK_TYPES.index(attack_type) / (len(ATTACK_TYPES) - 1)\n        \n        # Generate state vector\n        state = [\n            attack_val,                           # [0] attack_type\n            severity_val,                         # [1] severity\n            random.random(),                      # [2] source_port\n            random.random() * 0.2,                # [3] target_port\n            random.random() * severity_val,       # [4] packets_per_sec\n            random.random(),                      # [5] duration\n            1.0 if random.random() > 0.7 else 0.0,  # [6] is_known_attacker\n            random.random() * 0.5 + severity_val * 0.5,  # [7] device_risk\n            random.random(),                      # [8] time_of_day\n            random.random() * 0.3 + 0.2          # [9] protocol_risk\n        ]\n        \n        # Choose action: 80% optimal, 20% random (for exploration)\n        if random.random() > 0.2:\n            action = optimal_action\n        else:\n            action = random.choice(ACTIONS)\n        \n        action_id = ACTIONS.index(action)\n        is_optimal = (action == optimal_action)\n        \n        events.append({\n            'event_id': f'evt-{i:05d}',\n            'timestamp': (base_time - timedelta(minutes=i*5)).isoformat(),\n            'source_ip': f'192.168.1.{random.randint(100, 200)}',\n            'target_ip': '192.168.1.1',\n            'attack_type': attack_type,\n            'severity': severity,\n            'state_vector': state,\n            'action_taken': action,\n            'action_id': action_id,\n            'optimal_action': optimal_action,\n            'outcome_success': is_optimal,\n            'metadata': {'is_optimal': is_optimal}\n        })\n    \n    return events\n\n# Generate events\nUSE_SYNTHETIC = True\n\nif USE_SYNTHETIC:\n    events = generate_synthetic_events(15000)  # More data for better learning\n    print(f\"‚úÖ Generated {len(events)} synthetic events\")\n    \n    # Show distributions\n    from collections import Counter\n    \n    # Action distribution\n    action_counts = Counter([e['action_taken'] for e in events])\n    print(f\"\\nüìä Action Distribution (should be balanced):\")\n    for action in ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']:\n        count = action_counts.get(action, 0)\n        print(f\"   {action}: {count} ({100*count/len(events):.1f}%)\")\n    \n    # Optimal action rate\n    optimal_count = sum(1 for e in events if e['metadata']['is_optimal'])\n    print(f\"\\nüìä Optimal Action Rate: {100*optimal_count/len(events):.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTION B: Upload Real Events from Jetson\n",
    "# =============================================================================\n",
    "# Run this cell if you have real events exported from RAKSHAK\n",
    "\n",
    "UPLOAD_EVENTS = False  # Set to True to upload\n",
    "\n",
    "if UPLOAD_EVENTS:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"üì§ Upload your events JSON file(s):\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    events = []\n",
    "    for filename, content in uploaded.items():\n",
    "        data = json.loads(content.decode('utf-8'))\n",
    "        if isinstance(data, dict) and 'events' in data:\n",
    "            events.extend(data['events'])\n",
    "        elif isinstance(data, list):\n",
    "            events.extend(data)\n",
    "        print(f\"   Loaded {len(events)} events from {filename}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total events loaded: {len(events)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Reward Computation & Transition Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EXCLUSIVE OPTIMAL MAPPING (must match Cell 7!)\n# =============================================================================\nEXCLUSIVE_OPTIMAL = {\n    # MONITOR scenarios\n    ('port_scan', 'low'): 'MONITOR',\n    ('ping_sweep', 'low'): 'MONITOR',\n    \n    # DEPLOY_HONEYPOT scenarios\n    ('brute_force', 'low'): 'DEPLOY_HONEYPOT',\n    ('brute_force', 'medium'): 'DEPLOY_HONEYPOT',\n    \n    # ISOLATE_DEVICE scenarios\n    ('dos_attack', 'critical'): 'ISOLATE_DEVICE',\n    ('dos_attack', 'high'): 'ISOLATE_DEVICE',\n    ('malware', 'critical'): 'ISOLATE_DEVICE',\n    ('malware', 'high'): 'ISOLATE_DEVICE',\n    ('exploit_attempt', 'critical'): 'ISOLATE_DEVICE',\n    ('exploit_attempt', 'high'): 'ISOLATE_DEVICE',\n    \n    # ENGAGE_ATTACKER scenarios\n    ('dos_attack', 'medium'): 'ENGAGE_ATTACKER',\n    ('exploit_attempt', 'medium'): 'ENGAGE_ATTACKER',\n    ('malware', 'medium'): 'ENGAGE_ATTACKER',\n    \n    # ALERT_USER scenarios\n    ('data_exfiltration', 'low'): 'ALERT_USER',\n    ('data_exfiltration', 'medium'): 'ALERT_USER',\n    ('unknown_device', 'low'): 'ALERT_USER',\n    ('unknown_device', 'medium'): 'ALERT_USER',\n    ('unauthorized_access', 'low'): 'ALERT_USER',\n    ('unauthorized_access', 'medium'): 'ALERT_USER',\n}\n\n\ndef compute_reward(event: Dict) -> float:\n    \"\"\"\n    SIMPLIFIED reward function with STRONG differentiation.\n    \n    +10 for correct action\n    -5 for wrong action\n    \n    This 15-point gap creates clear Q-value separation.\n    \"\"\"\n    attack_type = event.get('attack_type', '')\n    severity = event.get('severity', 'medium')\n    action = event.get('action_taken', 'MONITOR')\n    \n    key = (attack_type, severity)\n    optimal_action = EXCLUSIVE_OPTIMAL.get(key)\n    \n    if optimal_action is None:\n        # Unknown combination - neutral reward\n        return 0.0\n    \n    if action == optimal_action:\n        return 10.0  # Strong positive\n    else:\n        return -5.0  # Strong negative\n\n\ndef build_transitions(events: List[Dict]) -> List[Transition]:\n    \"\"\"Convert events to RL transitions.\"\"\"\n    transitions = []\n    \n    for i, event in enumerate(events):\n        state = event.get('state_vector', [0.0] * 10)\n        if len(state) != 10:\n            continue\n        \n        state = np.array(state, dtype=np.float32)\n        action = event.get('action_id', 0)\n        reward = compute_reward(event)\n        \n        # Next state (small perturbation for offline RL)\n        if i + 1 < len(events):\n            next_state = events[i + 1].get('state_vector', [0.0] * 10)\n            done = False\n        else:\n            next_state = [0.0] * 10\n            done = True\n        \n        next_state = np.array(next_state, dtype=np.float32)\n        \n        transitions.append(Transition(\n            state=state,\n            action=action,\n            reward=reward,\n            next_state=next_state,\n            done=done\n        ))\n    \n    return transitions\n\n\n# Build transitions\ntransitions = build_transitions(events)\nprint(f\"‚úÖ Built {len(transitions)} transitions\")\n\n# Analyze rewards\nrewards = [t.reward for t in transitions]\nprint(f\"\\nüìä Reward Statistics:\")\nprint(f\"   Positive (+10): {sum(1 for r in rewards if r > 0)}\")\nprint(f\"   Negative (-5):  {sum(1 for r in rewards if r < 0)}\")\nprint(f\"   Neutral (0):    {sum(1 for r in rewards if r == 0)}\")\nprint(f\"   Mean: {np.mean(rewards):.2f}\")\n\n# Action distribution with rewards\naction_names = ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']\nprint(f\"\\nüìä Average Reward by Action:\")\nfor i, name in enumerate(action_names):\n    action_rewards = [t.reward for t in transitions if t.action == i]\n    if action_rewards:\n        avg = np.mean(action_rewards)\n        print(f\"   {name}: {avg:+.2f} (n={len(action_rewards)})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training Configuration\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'epochs': 200,           # Number of training epochs\n",
    "    'batch_size': 64,        # Batch size\n",
    "    'learning_rate': 0.001,  # Learning rate\n",
    "    'gamma': 0.99,           # Discount factor\n",
    "    'target_update': 10,     # Update target network every N epochs\n",
    "    'state_size': 10,        # State vector size\n",
    "    'action_size': 5,        # Number of actions\n",
    "    'hidden_size': 128       # Hidden layer size\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Training Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kaal(\n",
    "    transitions: List[Transition],\n",
    "    config: Dict,\n",
    "    device: torch.device\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train KAAL Dueling DQN using offline experience replay.\n",
    "    \n",
    "    Returns:\n",
    "        (policy_net, losses, best_loss)\n",
    "    \"\"\"\n",
    "    # Initialize networks\n",
    "    policy_net = DuelingDQN(\n",
    "        config['state_size'],\n",
    "        config['action_size'],\n",
    "        config['hidden_size']\n",
    "    ).to(device)\n",
    "    \n",
    "    target_net = DuelingDQN(\n",
    "        config['state_size'],\n",
    "        config['action_size'],\n",
    "        config['hidden_size']\n",
    "    ).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Load transitions into replay buffer\n",
    "    replay_buffer = ReplayBuffer(capacity=len(transitions) + 1000)\n",
    "    for t in transitions:\n",
    "        replay_buffer.push(t)\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting Training...\")\n",
    "    print(f\"   Transitions: {len(replay_buffer)}\")\n",
    "    print(f\"   Epochs: {config['epochs']}\")\n",
    "    print(f\"   Batch size: {config['batch_size']}\")\n",
    "    \n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    progress = tqdm(range(config['epochs']), desc=\"Training\")\n",
    "    \n",
    "    for epoch in progress:\n",
    "        epoch_losses = []\n",
    "        steps_per_epoch = max(1, len(transitions) // config['batch_size'])\n",
    "        \n",
    "        for step in range(steps_per_epoch):\n",
    "            # Sample batch\n",
    "            batch = replay_buffer.sample(config['batch_size'])\n",
    "            \n",
    "            # Prepare tensors\n",
    "            states = torch.FloatTensor([t.state for t in batch]).to(device)\n",
    "            actions = torch.LongTensor([t.action for t in batch]).to(device)\n",
    "            rewards = torch.FloatTensor([t.reward for t in batch]).to(device)\n",
    "            next_states = torch.FloatTensor([t.next_state for t in batch]).to(device)\n",
    "            dones = torch.FloatTensor([float(t.done) for t in batch]).to(device)\n",
    "            \n",
    "            # Compute Q(s, a)\n",
    "            current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "            \n",
    "            # Double DQN: select actions with policy, evaluate with target\n",
    "            with torch.no_grad():\n",
    "                next_actions = policy_net(next_states).argmax(1, keepdim=True)\n",
    "                next_q = target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "                target_q = rewards + (1 - dones) * config['gamma'] * next_q\n",
    "            \n",
    "            # Huber loss for stability\n",
    "            loss = F.smooth_l1_loss(current_q.squeeze(), target_q)\n",
    "            \n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        # Epoch stats\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        # Update target network\n",
    "        if (epoch + 1) % config['target_update'] == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        # Track best\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress.set_postfix({'loss': f'{avg_loss:.4f}', 'best': f'{best_loss:.4f}'})\n",
    "    \n",
    "    return policy_net, losses, best_loss\n",
    "\n",
    "\n",
    "# Train!\n",
    "policy_net, losses, best_loss = train_kaal(transitions, CONFIG, device)\n",
    "\n",
    "print(f\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"   Final Loss: {losses[-1]:.4f}\")\n",
    "print(f\"   Best Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, 'b-', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('KAAL Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot smoothed loss\n",
    "window = 10\n",
    "smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(smoothed, 'g-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Smoothed Loss')\n",
    "plt.title(f'KAAL Training Loss (smoothed, window={window})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Save & Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "checkpoint = {\n",
    "    'policy_state_dict': policy_net.state_dict(),\n",
    "    'state_size': CONFIG['state_size'],\n",
    "    'action_size': CONFIG['action_size'],\n",
    "    'hidden_size': CONFIG['hidden_size'],\n",
    "    'training_info': {\n",
    "        'epochs': CONFIG['epochs'],\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'learning_rate': CONFIG['learning_rate'],\n",
    "        'gamma': CONFIG['gamma'],\n",
    "        'transitions': len(transitions),\n",
    "        'final_loss': losses[-1],\n",
    "        'best_loss': best_loss,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to Google Drive\n",
    "model_path = f\"{OUTPUT_DIR}/kaal_policy.pth\"\n",
    "torch.save(checkpoint, model_path)\n",
    "print(f\"‚úÖ Model saved to Google Drive: {model_path}\")\n",
    "\n",
    "# Also save locally for download\n",
    "local_path = '/content/kaal_policy.pth'\n",
    "torch.save(checkpoint, local_path)\n",
    "print(f\"‚úÖ Model saved locally: {local_path}\")\n",
    "\n",
    "# Save inference-only version (smaller)\n",
    "inference_checkpoint = {\n",
    "    'policy_state_dict': policy_net.state_dict(),\n",
    "    'state_size': CONFIG['state_size'],\n",
    "    'action_size': CONFIG['action_size']\n",
    "}\n",
    "inference_path = f\"{OUTPUT_DIR}/kaal_policy_inference.pth\"\n",
    "torch.save(inference_checkpoint, inference_path)\n",
    "print(f\"‚úÖ Inference model saved: {inference_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading model...\")\n",
    "files.download(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Test the trained model\n# =============================================================================\nACTIONS = ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']\n\n# Attack type encoding (index / 8 for 9 attack types)\n# port_scan=0/8=0.0, brute_force=1/8=0.125, dos_attack=2/8=0.25, malware=3/8=0.375\n# exploit=4/8=0.5, data_exfil=5/8=0.625, unauthorized=6/8=0.75\n# ping_sweep=7/8=0.875, unknown_device=8/8=1.0\n\n# Severity encoding: low=0/3=0.0, medium=1/3=0.33, high=2/3=0.67, critical=3/3=1.0\n\ndef test_model(model, state_vector, device):\n    \"\"\"Test model on a state vector.\"\"\"\n    model.eval()\n    with torch.no_grad():\n        state = torch.FloatTensor(state_vector).unsqueeze(0).to(device)\n        q_values = model(state).cpu().numpy().flatten()\n        action = np.argmax(q_values)\n    return action, q_values\n\n\n# =============================================================================\n# TEST SCENARIOS - Aligned with EXCLUSIVE_OPTIMAL mapping\n# =============================================================================\ntest_scenarios = [\n    # =========================================================================\n    # MONITOR scenarios: (port_scan, low), (ping_sweep, low)\n    # =========================================================================\n    {\n        'name': 'üü¢ Port Scan (Low)',\n        'state': [0.0, 0.0, 0.3, 0.02, 0.0, 0.1, 0.0, 0.1, 0.5, 0.3],  # port_scan=0.0, low=0.0\n        'expected': 'MONITOR',\n        'attack_type': 'port_scan',\n        'severity': 'low'\n    },\n    {\n        'name': 'üü¢ Ping Sweep (Low)',\n        'state': [0.875, 0.0, 0.1, 0.01, 0.0, 0.05, 0.0, 0.05, 0.6, 0.1],  # ping_sweep=0.875, low=0.0\n        'expected': 'MONITOR',\n        'attack_type': 'ping_sweep',\n        'severity': 'low'\n    },\n\n    # =========================================================================\n    # DEPLOY_HONEYPOT scenarios: (brute_force, low), (brute_force, medium)\n    # =========================================================================\n    {\n        'name': 'üü° Brute Force (Low)',\n        'state': [0.125, 0.0, 0.2, 0.01, 0.0, 0.1, 0.0, 0.1, 0.8, 0.2],  # brute_force=0.125, low=0.0\n        'expected': 'DEPLOY_HONEYPOT',\n        'attack_type': 'brute_force',\n        'severity': 'low'\n    },\n    {\n        'name': 'üü° Brute Force (Medium)',\n        'state': [0.125, 0.33, 0.3, 0.02, 0.1, 0.2, 0.0, 0.3, 0.5, 0.3],  # brute_force=0.125, medium=0.33\n        'expected': 'DEPLOY_HONEYPOT',\n        'attack_type': 'brute_force',\n        'severity': 'medium'\n    },\n\n    # =========================================================================\n    # ISOLATE_DEVICE scenarios: (dos/malware/exploit, critical/high)\n    # =========================================================================\n    {\n        'name': 'üî¥ DoS Attack (Critical)',\n        'state': [0.25, 1.0, 0.5, 0.01, 0.9, 0.8, 1.0, 0.9, 0.3, 0.5],  # dos=0.25, critical=1.0\n        'expected': 'ISOLATE_DEVICE',\n        'attack_type': 'dos_attack',\n        'severity': 'critical'\n    },\n    {\n        'name': 'üî¥ Malware (Critical)',\n        'state': [0.375, 1.0, 0.2, 0.05, 0.3, 0.5, 1.0, 0.8, 0.7, 0.6],  # malware=0.375, critical=1.0\n        'expected': 'ISOLATE_DEVICE',\n        'attack_type': 'malware',\n        'severity': 'critical'\n    },\n    {\n        'name': 'üü† Exploit (High)',\n        'state': [0.5, 0.67, 0.4, 0.03, 0.5, 0.6, 1.0, 0.7, 0.2, 0.5],  # exploit=0.5, high=0.67\n        'expected': 'ISOLATE_DEVICE',\n        'attack_type': 'exploit_attempt',\n        'severity': 'high'\n    },\n\n    # =========================================================================\n    # ENGAGE_ATTACKER scenarios: (dos/exploit/malware, medium)\n    # =========================================================================\n    {\n        'name': 'üü° DoS Probe (Medium)',\n        'state': [0.25, 0.33, 0.7, 0.02, 0.3, 0.3, 0.0, 0.4, 0.1, 0.4],  # dos=0.25, medium=0.33\n        'expected': 'ENGAGE_ATTACKER',\n        'attack_type': 'dos_attack',\n        'severity': 'medium'\n    },\n    {\n        'name': 'üü° Exploit Attempt (Medium)',\n        'state': [0.5, 0.33, 0.6, 0.04, 0.3, 0.4, 0.0, 0.5, 0.3, 0.5],  # exploit=0.5, medium=0.33\n        'expected': 'ENGAGE_ATTACKER',\n        'attack_type': 'exploit_attempt',\n        'severity': 'medium'\n    },\n\n    # =========================================================================\n    # ALERT_USER scenarios: (data_exfil/unknown/unauthorized, low/medium)\n    # =========================================================================\n    {\n        'name': 'üîî Data Exfiltration (Medium)',\n        'state': [0.625, 0.33, 0.7, 0.06, 0.2, 0.4, 0.0, 0.5, 0.4, 0.3],  # data_exfil=0.625, medium=0.33\n        'expected': 'ALERT_USER',\n        'attack_type': 'data_exfiltration',\n        'severity': 'medium'\n    },\n    {\n        'name': 'üîî Unknown Device (Medium)',\n        'state': [1.0, 0.33, 0.5, 0.03, 0.1, 0.2, 0.0, 0.6, 0.9, 0.4],  # unknown=1.0, medium=0.33\n        'expected': 'ALERT_USER',\n        'attack_type': 'unknown_device',\n        'severity': 'medium'\n    },\n    {\n        'name': 'üîî Unauthorized Access (Low)',\n        'state': [0.75, 0.0, 0.4, 0.02, 0.0, 0.2, 0.0, 0.4, 0.7, 0.3],  # unauthorized=0.75, low=0.0\n        'expected': 'ALERT_USER',\n        'attack_type': 'unauthorized_access',\n        'severity': 'low'\n    }\n]\n\n# =============================================================================\n# Run Tests\n# =============================================================================\nprint(\"üß™ KAAL Model Test Suite (Aligned with EXCLUSIVE_OPTIMAL)\")\nprint(\"=\" * 70)\nprint(f\"Testing {len(test_scenarios)} scenarios\\n\")\n\nresults = {'pass': 0, 'fail': 0}\naction_results = {a: {'pass': 0, 'fail': 0} for a in ACTIONS}\n\nfor scenario in test_scenarios:\n    action_id, q_values = test_model(policy_net, scenario['state'], device)\n    predicted = ACTIONS[action_id]\n    expected = scenario['expected']\n    passed = predicted == expected\n    \n    status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n    results['pass' if passed else 'fail'] += 1\n    action_results[expected]['pass' if passed else 'fail'] += 1\n    \n    print(f\"\\n{scenario['name']}\")\n    print(f\"   ({scenario['attack_type']}, {scenario['severity']})\")\n    print(f\"   Expected: {expected}\")\n    print(f\"   Got:      {predicted} {status}\")\n    print(f\"   Q-values:\")\n    for i, (name, q) in enumerate(zip(ACTIONS, q_values)):\n        marker = \" ‚Üê CHOSEN\" if i == action_id else \"\"\n        exp_marker = \" (expected)\" if name == expected else \"\"\n        print(f\"      {name:20s}: {q:8.3f}{marker}{exp_marker}\")\n\n# =============================================================================\n# Summary\n# =============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"üìä TEST SUMMARY\")\nprint(\"=\" * 70)\ntotal = results['pass'] + results['fail']\nprint(f\"\\nOverall: {results['pass']}/{total} passed ({100*results['pass']/total:.1f}%)\")\n\nprint(\"\\nBy Action:\")\nfor action in ACTIONS:\n    p = action_results[action]['pass']\n    f = action_results[action]['fail']\n    total_action = p + f\n    if total_action > 0:\n        pct = 100 * p / total_action\n        status = \"‚úÖ\" if pct == 100 else \"‚ö†Ô∏è\" if pct >= 50 else \"‚ùå\"\n        print(f\"   {status} {action:20s}: {p}/{total_action} ({pct:.0f}%)\")\n\nprint(\"\\n\" + \"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Deployment Instructions\n",
    "\n",
    "### Copy to Jetson\n",
    "```bash\n",
    "scp kaal_policy.pth user@jetson-ip:~/e-raksha/models/\n",
    "```\n",
    "\n",
    "### Verify on Jetson\n",
    "```python\n",
    "from core.agentic_defender import AgenticDefender\n",
    "import yaml\n",
    "\n",
    "with open('config/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update config to use new model\n",
    "config['agent']['model_path'] = 'models/kaal_policy.pth'\n",
    "\n",
    "agent = AgenticDefender(config)\n",
    "print(f'Model loaded: {agent.model_loaded}')\n",
    "print(f'Mode: {agent.get_statistics()[\"mode\"]}')\n",
    "```\n",
    "\n",
    "### Run RAKSHAK\n",
    "```bash\n",
    "sudo python main.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}