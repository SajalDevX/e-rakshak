{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›¡ï¸ KAAL Offline RL Training Notebook\n",
    "\n",
    "## RAKSHAK - Agentic AI Cyber Guardian\n",
    "\n",
    "This notebook trains the **KAAL (Knowledge-Augmented Autonomous Learner)** Dueling DQN agent for autonomous cyber defense.\n",
    "\n",
    "### What is KAAL?\n",
    "KAAL is a reinforcement learning agent that decides defensive actions against cyber threats:\n",
    "- **MONITOR** - Continue observing the threat\n",
    "- **DEPLOY_HONEYPOT** - Deploy a decoy to gather intelligence\n",
    "- **ISOLATE_DEVICE** - Quarantine the compromised device\n",
    "- **ENGAGE_ATTACKER** - Redirect attacker to honeypot\n",
    "- **ALERT_USER** - Send notification to user\n",
    "\n",
    "### Training Approach\n",
    "- **Offline RL**: Train from stored attack events (no live environment needed)\n",
    "- **Dueling DQN**: Separates value and advantage for better learning\n",
    "- **Experience Replay**: Random sampling for stable training\n",
    "\n",
    "### Output\n",
    "- `kaal_policy.pth` - Trained model for Jetson deployment\n",
    "\n",
    "---\n",
    "**Author**: Team RAKSHAK  \n",
    "**Runtime**: GPU recommended (T4 or better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy matplotlib tqdm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving model)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/RAKSHAK_Models'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Dueling DQN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling Deep Q-Network architecture.\n",
    "    \n",
    "    Separates the network into value and advantage streams:\n",
    "    Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\n",
    "    \n",
    "    This helps the agent learn which states are valuable\n",
    "    without having to learn the effect of each action.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int = 10, action_size: int = 5, hidden_size: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Shared feature extraction\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Value stream - estimates V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream - estimates A(s, a)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, action_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.feature(x)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        \n",
    "        # Combine: Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    \"\"\"RL transition tuple.\"\"\"\n",
    "    state: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    done: bool\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, transition: Transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size: int) -> List[Transition]:\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Test the architecture\n",
    "model = DuelingDQN().to(device)\n",
    "test_input = torch.randn(1, 10).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"âœ… DuelingDQN created successfully!\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "print(f\"   Output shape: {test_output.shape}\")\n",
    "print(f\"   Q-values: {test_output.detach().cpu().numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Training Data\n",
    "\n",
    "Choose one of the options below:\n",
    "- **Option A**: Generate synthetic training data (for testing)\n",
    "- **Option B**: Upload real events from Jetson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# OPTION A: Generate Synthetic Training Data\n# =============================================================================\n# Use this for testing the training pipeline\n\ndef generate_synthetic_events(num_events: int = 5000) -> List[Dict]:\n    \"\"\"\n    Generate synthetic attack events for training.\n    \n    Uses SEVERITY + ATTACK_TYPE based optimal action selection.\n    Now properly includes MONITOR and ALERT_USER scenarios.\n    \"\"\"\n    ATTACK_TYPES = ['port_scan', 'brute_force', 'dos_attack', 'malware', \n                    'exploit_attempt', 'data_exfiltration', 'unauthorized_access',\n                    'ping_sweep', 'unknown_device']  # Added new types\n    SEVERITIES = ['low', 'medium', 'high', 'critical']\n    ACTIONS = ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']\n    \n    # ==========================================================================\n    # TWO-TIER OPTIMAL ACTION MAPPING\n    # ==========================================================================\n    # Tier 1: Severity-based (general rule)\n    OPTIMAL_BY_SEVERITY = {\n        'critical': ['ISOLATE_DEVICE'],\n        'high': ['ISOLATE_DEVICE', 'ENGAGE_ATTACKER'],\n        'medium': ['ENGAGE_ATTACKER', 'DEPLOY_HONEYPOT'],\n        'low': ['MONITOR']  # CHANGED: Only MONITOR for low severity\n    }\n    \n    # Tier 2: Attack-type overrides (specific scenarios)\n    # These override severity-based decisions for specific attack types\n    ALERT_USER_ATTACKS = ['data_exfiltration', 'unknown_device', 'unauthorized_access']\n    MONITOR_ONLY_ATTACKS = ['ping_sweep', 'port_scan']  # Very low risk, just observe\n    HONEYPOT_ATTACKS = ['brute_force']  # Good for credential harvesting\n    \n    events = []\n    base_time = datetime.now()\n    \n    for i in range(num_events):\n        attack_type = random.choice(ATTACK_TYPES)\n        severity = random.choice(SEVERITIES)\n        \n        # Severity affects state vector\n        severity_val = SEVERITIES.index(severity) / 3.0\n        attack_val = ATTACK_TYPES.index(attack_type) / (len(ATTACK_TYPES) - 1)\n        \n        # Generate realistic state vector\n        state = [\n            attack_val,                           # attack_type\n            severity_val,                         # severity (0=low, 1=critical)\n            random.random(),                      # source_port (normalized)\n            random.random() * 0.2,                # target_port (typically low)\n            random.random() * severity_val,       # packets_per_sec\n            random.random(),                      # duration\n            1.0 if random.random() > 0.7 else 0.0,  # is_known_attacker\n            random.random() * 0.5 + severity_val * 0.5,  # device_risk\n            random.random(),                      # time_of_day\n            random.random() * 0.3 + 0.2          # protocol_risk\n        ]\n        \n        # =======================================================================\n        # DETERMINE OPTIMAL ACTION (severity + attack_type)\n        # =======================================================================\n        if random.random() > 0.3:\n            # 70% of time: choose optimal action\n            \n            # Priority 1: ALERT_USER for specific attack types (any severity)\n            if attack_type in ALERT_USER_ATTACKS and severity in ['medium', 'low']:\n                optimal_actions = ['ALERT_USER']\n            \n            # Priority 2: MONITOR only for very low-risk scans\n            elif attack_type in MONITOR_ONLY_ATTACKS and severity == 'low':\n                optimal_actions = ['MONITOR']\n            \n            # Priority 3: DEPLOY_HONEYPOT for brute force (credential capture)\n            elif attack_type in HONEYPOT_ATTACKS and severity in ['low', 'medium']:\n                optimal_actions = ['DEPLOY_HONEYPOT']\n            \n            # Default: Use severity-based mapping\n            else:\n                optimal_actions = OPTIMAL_BY_SEVERITY.get(severity, ACTIONS)\n            \n            action = random.choice(optimal_actions)\n        else:\n            # 30% exploration: random action\n            action = random.choice(ACTIONS)\n        \n        action_id = ACTIONS.index(action)\n        \n        # Determine if action is optimal for this scenario\n        is_optimal = False\n        if attack_type in ALERT_USER_ATTACKS and severity in ['medium', 'low']:\n            is_optimal = action == 'ALERT_USER'\n        elif attack_type in MONITOR_ONLY_ATTACKS and severity == 'low':\n            is_optimal = action == 'MONITOR'\n        elif attack_type in HONEYPOT_ATTACKS and severity in ['low', 'medium']:\n            is_optimal = action == 'DEPLOY_HONEYPOT'\n        else:\n            is_optimal = action in OPTIMAL_BY_SEVERITY.get(severity, [])\n        \n        success = is_optimal or random.random() > 0.5\n        \n        events.append({\n            'event_id': f'evt-{i:05d}',\n            'timestamp': (base_time - timedelta(minutes=i*5)).isoformat(),\n            'source_ip': f'192.168.1.{random.randint(100, 200)}',\n            'target_ip': '192.168.1.1',\n            'attack_type': attack_type,\n            'severity': severity,\n            'state_vector': state,\n            'action_taken': action,\n            'action_id': action_id,\n            'outcome': 'blocked' if action == 'ISOLATE_DEVICE' else 'handled',\n            'outcome_success': success,\n            'metadata': {\n                'ttp_captured': success and action == 'ENGAGE_ATTACKER',\n                'is_optimal': is_optimal\n            }\n        })\n    \n    return events\n\n# Generate events\nUSE_SYNTHETIC = True  # Set to False if uploading real events\n\nif USE_SYNTHETIC:\n    events = generate_synthetic_events(10000)\n    print(f\"âœ… Generated {len(events)} synthetic events\")\n    print(f\"   Sample event: {events[0]['attack_type']} ({events[0]['severity']}) -> {events[0]['action_taken']}\")\n    \n    # Show distributions\n    from collections import Counter\n    \n    sev_counts = Counter([e['severity'] for e in events])\n    print(f\"\\nðŸ“Š Severity Distribution:\")\n    for sev in ['low', 'medium', 'high', 'critical']:\n        print(f\"   {sev}: {sev_counts[sev]} ({100*sev_counts[sev]/len(events):.1f}%)\")\n    \n    action_counts = Counter([e['action_taken'] for e in events])\n    print(f\"\\nðŸ“Š Action Distribution:\")\n    for action in ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']:\n        print(f\"   {action}: {action_counts[action]} ({100*action_counts[action]/len(events):.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTION B: Upload Real Events from Jetson\n",
    "# =============================================================================\n",
    "# Run this cell if you have real events exported from RAKSHAK\n",
    "\n",
    "UPLOAD_EVENTS = False  # Set to True to upload\n",
    "\n",
    "if UPLOAD_EVENTS:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"ðŸ“¤ Upload your events JSON file(s):\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    events = []\n",
    "    for filename, content in uploaded.items():\n",
    "        data = json.loads(content.decode('utf-8'))\n",
    "        if isinstance(data, dict) and 'events' in data:\n",
    "            events.extend(data['events'])\n",
    "        elif isinstance(data, list):\n",
    "            events.extend(data)\n",
    "        print(f\"   Loaded {len(events)} events from {filename}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Total events loaded: {len(events)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Reward Computation & Transition Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_reward(event: Dict) -> float:\n    \"\"\"\n    Compute reward based on SEVERITY + ATTACK_TYPE coupling.\n    \n    Two-tier reward system:\n    - Tier 1: Severity-based optimal actions\n    - Tier 2: Attack-type specific overrides\n    \n    This ensures MONITOR and ALERT_USER get proper rewards.\n    \"\"\"\n    severity = event.get('severity', 'medium')\n    action = event.get('action_taken', 'MONITOR')\n    success = event.get('outcome_success', True)\n    attack_type = event.get('attack_type', '')\n    \n    # ==========================================================================\n    # TIER 1: Severity-based optimal actions\n    # ==========================================================================\n    OPTIMAL_BY_SEVERITY = {\n        'critical': ['ISOLATE_DEVICE'],\n        'high': ['ISOLATE_DEVICE', 'ENGAGE_ATTACKER'],\n        'medium': ['ENGAGE_ATTACKER', 'DEPLOY_HONEYPOT'],\n        'low': ['MONITOR']  # Only MONITOR for low\n    }\n    \n    # ==========================================================================\n    # TIER 2: Attack-type overrides\n    # ==========================================================================\n    ALERT_USER_ATTACKS = ['data_exfiltration', 'unknown_device', 'unauthorized_access']\n    MONITOR_ONLY_ATTACKS = ['ping_sweep', 'port_scan']\n    HONEYPOT_ATTACKS = ['brute_force']\n    \n    # Base reward scales with severity\n    BASE_REWARDS = {\n        'critical': 10.0,\n        'high': 5.0,\n        'medium': 2.0,\n        'low': 1.0  # Increased from 0.5 to make MONITOR more learnable\n    }\n    \n    base = BASE_REWARDS.get(severity, 1.0)\n    \n    # ==========================================================================\n    # DETERMINE IF ACTION IS OPTIMAL\n    # ==========================================================================\n    is_optimal = False\n    \n    # Priority 1: ALERT_USER for specific attack types\n    if attack_type in ALERT_USER_ATTACKS and severity in ['medium', 'low']:\n        is_optimal = action == 'ALERT_USER'\n        if is_optimal:\n            base = 3.0  # Boost reward for ALERT_USER scenarios\n    \n    # Priority 2: MONITOR only for very low-risk scans\n    elif attack_type in MONITOR_ONLY_ATTACKS and severity == 'low':\n        is_optimal = action == 'MONITOR'\n        if is_optimal:\n            base = 2.0  # Boost reward for proper MONITOR usage\n    \n    # Priority 3: DEPLOY_HONEYPOT for brute force\n    elif attack_type in HONEYPOT_ATTACKS and severity in ['low', 'medium']:\n        is_optimal = action == 'DEPLOY_HONEYPOT'\n    \n    # Default: Use severity-based mapping\n    else:\n        is_optimal = action in OPTIMAL_BY_SEVERITY.get(severity, [])\n    \n    # ==========================================================================\n    # COMPUTE FINAL REWARD\n    # ==========================================================================\n    if success:\n        if is_optimal:\n            reward = base * 2.0\n            \n            # Bonus for TTP capture\n            if event.get('metadata', {}).get('ttp_captured'):\n                reward += 2.0\n        else:\n            # Suboptimal action - penalty depends on severity\n            penalty = 0.2 if severity in ['critical', 'high'] else 0.4\n            reward = base * penalty\n    else:\n        # Failed action - negative reward\n        reward = -base\n    \n    return reward\n\n\ndef build_transitions(events: List[Dict]) -> List[Transition]:\n    \"\"\"\n    Convert events to RL transitions.\n    \n    Each event becomes (state, action, reward, next_state, done).\n    \"\"\"\n    transitions = []\n    \n    for i, event in enumerate(events):\n        state = event.get('state_vector', [0.0] * 10)\n        if len(state) != 10:\n            continue\n        \n        state = np.array(state, dtype=np.float32)\n        action = event.get('action_id', 0)\n        reward = compute_reward(event)\n        \n        # Get next state\n        if i + 1 < len(events):\n            next_state = events[i + 1].get('state_vector', [0.0] * 10)\n            done = False\n        else:\n            next_state = [0.0] * 10\n            done = True\n        \n        next_state = np.array(next_state, dtype=np.float32)\n        \n        transitions.append(Transition(\n            state=state,\n            action=action,\n            reward=reward,\n            next_state=next_state,\n            done=done\n        ))\n    \n    return transitions\n\n\n# Build transitions\ntransitions = build_transitions(events)\nprint(f\"âœ… Built {len(transitions)} transitions\")\n\n# Analyze rewards\nrewards = [t.reward for t in transitions]\nprint(f\"\\nðŸ“Š Reward Statistics:\")\nprint(f\"   Mean: {np.mean(rewards):.2f}\")\nprint(f\"   Std:  {np.std(rewards):.2f}\")\nprint(f\"   Min:  {np.min(rewards):.2f}\")\nprint(f\"   Max:  {np.max(rewards):.2f}\")\n\n# Action distribution\nactions = [t.action for t in transitions]\naction_names = ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']\nprint(f\"\\nðŸ“Š Action Distribution:\")\nfor i, name in enumerate(action_names):\n    count = actions.count(i)\n    pct = 100*count/len(actions) if len(actions) > 0 else 0\n    print(f\"   {name}: {count} ({pct:.1f}%)\")\n\n# Show reward by action\nprint(f\"\\nðŸ“Š Average Reward by Action:\")\nfor i, name in enumerate(action_names):\n    action_rewards = [t.reward for t in transitions if t.action == i]\n    if action_rewards:\n        print(f\"   {name}: {np.mean(action_rewards):.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training Configuration\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'epochs': 200,           # Number of training epochs\n",
    "    'batch_size': 64,        # Batch size\n",
    "    'learning_rate': 0.001,  # Learning rate\n",
    "    'gamma': 0.99,           # Discount factor\n",
    "    'target_update': 10,     # Update target network every N epochs\n",
    "    'state_size': 10,        # State vector size\n",
    "    'action_size': 5,        # Number of actions\n",
    "    'hidden_size': 128       # Hidden layer size\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ Training Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kaal(\n",
    "    transitions: List[Transition],\n",
    "    config: Dict,\n",
    "    device: torch.device\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train KAAL Dueling DQN using offline experience replay.\n",
    "    \n",
    "    Returns:\n",
    "        (policy_net, losses, best_loss)\n",
    "    \"\"\"\n",
    "    # Initialize networks\n",
    "    policy_net = DuelingDQN(\n",
    "        config['state_size'],\n",
    "        config['action_size'],\n",
    "        config['hidden_size']\n",
    "    ).to(device)\n",
    "    \n",
    "    target_net = DuelingDQN(\n",
    "        config['state_size'],\n",
    "        config['action_size'],\n",
    "        config['hidden_size']\n",
    "    ).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Load transitions into replay buffer\n",
    "    replay_buffer = ReplayBuffer(capacity=len(transitions) + 1000)\n",
    "    for t in transitions:\n",
    "        replay_buffer.push(t)\n",
    "    \n",
    "    print(f\"\\nðŸš€ Starting Training...\")\n",
    "    print(f\"   Transitions: {len(replay_buffer)}\")\n",
    "    print(f\"   Epochs: {config['epochs']}\")\n",
    "    print(f\"   Batch size: {config['batch_size']}\")\n",
    "    \n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    progress = tqdm(range(config['epochs']), desc=\"Training\")\n",
    "    \n",
    "    for epoch in progress:\n",
    "        epoch_losses = []\n",
    "        steps_per_epoch = max(1, len(transitions) // config['batch_size'])\n",
    "        \n",
    "        for step in range(steps_per_epoch):\n",
    "            # Sample batch\n",
    "            batch = replay_buffer.sample(config['batch_size'])\n",
    "            \n",
    "            # Prepare tensors\n",
    "            states = torch.FloatTensor([t.state for t in batch]).to(device)\n",
    "            actions = torch.LongTensor([t.action for t in batch]).to(device)\n",
    "            rewards = torch.FloatTensor([t.reward for t in batch]).to(device)\n",
    "            next_states = torch.FloatTensor([t.next_state for t in batch]).to(device)\n",
    "            dones = torch.FloatTensor([float(t.done) for t in batch]).to(device)\n",
    "            \n",
    "            # Compute Q(s, a)\n",
    "            current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "            \n",
    "            # Double DQN: select actions with policy, evaluate with target\n",
    "            with torch.no_grad():\n",
    "                next_actions = policy_net(next_states).argmax(1, keepdim=True)\n",
    "                next_q = target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "                target_q = rewards + (1 - dones) * config['gamma'] * next_q\n",
    "            \n",
    "            # Huber loss for stability\n",
    "            loss = F.smooth_l1_loss(current_q.squeeze(), target_q)\n",
    "            \n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        # Epoch stats\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        # Update target network\n",
    "        if (epoch + 1) % config['target_update'] == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        # Track best\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress.set_postfix({'loss': f'{avg_loss:.4f}', 'best': f'{best_loss:.4f}'})\n",
    "    \n",
    "    return policy_net, losses, best_loss\n",
    "\n",
    "\n",
    "# Train!\n",
    "policy_net, losses, best_loss = train_kaal(transitions, CONFIG, device)\n",
    "\n",
    "print(f\"\\nâœ… Training Complete!\")\n",
    "print(f\"   Final Loss: {losses[-1]:.4f}\")\n",
    "print(f\"   Best Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, 'b-', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('KAAL Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot smoothed loss\n",
    "window = 10\n",
    "smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(smoothed, 'g-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Smoothed Loss')\n",
    "plt.title(f'KAAL Training Loss (smoothed, window={window})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Save & Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "checkpoint = {\n",
    "    'policy_state_dict': policy_net.state_dict(),\n",
    "    'state_size': CONFIG['state_size'],\n",
    "    'action_size': CONFIG['action_size'],\n",
    "    'hidden_size': CONFIG['hidden_size'],\n",
    "    'training_info': {\n",
    "        'epochs': CONFIG['epochs'],\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'learning_rate': CONFIG['learning_rate'],\n",
    "        'gamma': CONFIG['gamma'],\n",
    "        'transitions': len(transitions),\n",
    "        'final_loss': losses[-1],\n",
    "        'best_loss': best_loss,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to Google Drive\n",
    "model_path = f\"{OUTPUT_DIR}/kaal_policy.pth\"\n",
    "torch.save(checkpoint, model_path)\n",
    "print(f\"âœ… Model saved to Google Drive: {model_path}\")\n",
    "\n",
    "# Also save locally for download\n",
    "local_path = '/content/kaal_policy.pth'\n",
    "torch.save(checkpoint, local_path)\n",
    "print(f\"âœ… Model saved locally: {local_path}\")\n",
    "\n",
    "# Save inference-only version (smaller)\n",
    "inference_checkpoint = {\n",
    "    'policy_state_dict': policy_net.state_dict(),\n",
    "    'state_size': CONFIG['state_size'],\n",
    "    'action_size': CONFIG['action_size']\n",
    "}\n",
    "inference_path = f\"{OUTPUT_DIR}/kaal_policy_inference.pth\"\n",
    "torch.save(inference_checkpoint, inference_path)\n",
    "print(f\"âœ… Inference model saved: {inference_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ðŸ“¥ Downloading model...\")\n",
    "files.download(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the trained model with COMPREHENSIVE test cases\nACTIONS = ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']\n\n# Attack type encoding (must match training data!)\n# 0: port_scan=0.0, 1: brute_force=0.125, 2: dos_attack=0.25, 3: malware=0.375\n# 4: exploit_attempt=0.5, 5: data_exfiltration=0.625, 6: unauthorized_access=0.75\n# 7: ping_sweep=0.875, 8: unknown_device=1.0\n\ndef test_model(model, state_vector, device):\n    \"\"\"Test model on a state vector.\"\"\"\n    model.eval()\n    with torch.no_grad():\n        state = torch.FloatTensor(state_vector).unsqueeze(0).to(device)\n        q_values = model(state).cpu().numpy().flatten()\n        action = np.argmax(q_values)\n    return action, q_values\n\n\n# =============================================================================\n# COMPREHENSIVE TEST SCENARIOS - All 5 Actions Covered\n# =============================================================================\n# State vector: [attack_type, severity, src_port, tgt_port, pps, duration, \n#                known_attacker, device_risk, time_of_day, protocol_risk]\n# Severity: 0.0=low, 0.33=medium, 0.67=high, 1.0=critical\n# Attack types: port_scan=0.0, brute_force=0.125, dos_attack=0.25, malware=0.375,\n#               exploit=0.5, data_exfil=0.625, unauth=0.75, ping=0.875, unknown=1.0\n\ntest_scenarios = [\n    # =========================================================================\n    # ISOLATE_DEVICE scenarios (critical/high severity - immediate containment)\n    # =========================================================================\n    {\n        'name': 'ðŸ”´ Critical DDoS Attack',\n        'state': [0.25, 1.0, 0.5, 0.01, 0.9, 0.8, 1.0, 0.9, 0.3, 0.5],  # dos_attack, critical\n        'expected': 'ISOLATE_DEVICE',\n        'severity': 'critical',\n        'rationale': 'Critical DDoS - must isolate immediately to stop damage'\n    },\n    {\n        'name': 'ðŸ”´ Critical Malware Detected',\n        'state': [0.375, 1.0, 0.2, 0.05, 0.3, 0.5, 1.0, 0.8, 0.7, 0.6],  # malware, critical\n        'expected': 'ISOLATE_DEVICE',\n        'severity': 'critical',\n        'rationale': 'Malware spreading - quarantine device to prevent lateral movement'\n    },\n    {\n        'name': 'ðŸŸ  High Severity Exploit',\n        'state': [0.5, 0.67, 0.4, 0.03, 0.7, 0.6, 1.0, 0.7, 0.2, 0.5],  # exploit, high\n        'expected': 'ISOLATE_DEVICE',\n        'severity': 'high',\n        'rationale': 'Active exploit - isolate before compromise spreads'\n    },\n\n    # =========================================================================\n    # ENGAGE_ATTACKER scenarios (medium severity - intel gathering opportunity)\n    # =========================================================================\n    {\n        'name': 'ðŸŸ¡ Medium Exploit Attempt',\n        'state': [0.5, 0.33, 0.6, 0.04, 0.5, 0.4, 0.0, 0.5, 0.3, 0.5],  # exploit, medium\n        'expected': 'ENGAGE_ATTACKER',\n        'severity': 'medium',\n        'rationale': 'Exploit attempt - engage to capture attack payloads'\n    },\n    {\n        'name': 'ðŸŸ¡ Medium DDoS Probe',\n        'state': [0.25, 0.33, 0.7, 0.02, 0.4, 0.3, 0.0, 0.4, 0.1, 0.4],  # dos_attack, medium\n        'expected': 'ENGAGE_ATTACKER',\n        'severity': 'medium',\n        'rationale': 'Medium DDoS - redirect to honeypot to capture TTPs'\n    },\n\n    # =========================================================================\n    # DEPLOY_HONEYPOT scenarios (brute force - credential capture)\n    # =========================================================================\n    {\n        'name': 'ðŸŸ¢ Low Brute Force Attempt',\n        'state': [0.125, 0.0, 0.2, 0.01, 0.05, 0.1, 0.0, 0.1, 0.8, 0.2],  # brute_force, low\n        'expected': 'DEPLOY_HONEYPOT',\n        'severity': 'low',\n        'rationale': 'Brute force - honeypot to capture credentials'\n    },\n    {\n        'name': 'ðŸŸ¡ Medium Brute Force',\n        'state': [0.125, 0.33, 0.3, 0.02, 0.2, 0.2, 0.0, 0.3, 0.5, 0.3],  # brute_force, medium\n        'expected': 'DEPLOY_HONEYPOT',\n        'severity': 'medium',\n        'rationale': 'Credential attack - deploy honeypot for credential harvesting'\n    },\n\n    # =========================================================================\n    # MONITOR scenarios (low severity port scan / ping sweep)\n    # =========================================================================\n    {\n        'name': 'ðŸŸ¢ Low Port Scan',\n        'state': [0.0, 0.0, 0.3, 0.02, 0.1, 0.1, 0.0, 0.2, 0.5, 0.3],  # port_scan, low\n        'expected': 'MONITOR',\n        'severity': 'low',\n        'rationale': 'Routine port scan - just monitor, no action needed'\n    },\n    {\n        'name': 'ðŸŸ¢ Ping Sweep',\n        'state': [0.875, 0.0, 0.1, 0.01, 0.02, 0.05, 0.0, 0.1, 0.6, 0.1],  # ping_sweep, low\n        'expected': 'MONITOR',\n        'severity': 'low',\n        'rationale': 'Basic ping sweep - observe only, minimal threat'\n    },\n\n    # =========================================================================\n    # ALERT_USER scenarios (data exfiltration, unknown device, unauthorized)\n    # =========================================================================\n    {\n        'name': 'ðŸ”” Unknown Device Connected',\n        'state': [1.0, 0.33, 0.5, 0.03, 0.1, 0.2, 0.0, 0.6, 0.9, 0.4],  # unknown_device, medium\n        'expected': 'ALERT_USER',\n        'severity': 'medium',\n        'rationale': 'New unknown device - user must verify if legitimate'\n    },\n    {\n        'name': 'ðŸ”” Data Exfiltration Detected',\n        'state': [0.625, 0.33, 0.7, 0.06, 0.3, 0.4, 0.0, 0.5, 0.4, 0.3],  # data_exfil, medium\n        'expected': 'ALERT_USER',\n        'severity': 'medium',\n        'rationale': 'Data leaving network - user needs to investigate immediately'\n    },\n    {\n        'name': 'ðŸ”” Unauthorized Access Attempt',\n        'state': [0.75, 0.0, 0.4, 0.02, 0.15, 0.2, 0.0, 0.4, 0.7, 0.3],  # unauthorized, low\n        'expected': 'ALERT_USER',\n        'severity': 'low',\n        'rationale': 'Unauthorized access - alert user to verify credentials'\n    }\n]\n\n# =============================================================================\n# Run Tests\n# =============================================================================\nprint(\"ðŸ§ª KAAL Model Comprehensive Test Suite\")\nprint(\"=\" * 70)\nprint(f\"Testing {len(test_scenarios)} scenarios across all 5 actions\\n\")\n\nresults = {'pass': 0, 'fail': 0}\naction_results = {a: {'pass': 0, 'fail': 0} for a in ACTIONS}\n\nfor scenario in test_scenarios:\n    action_id, q_values = test_model(policy_net, scenario['state'], device)\n    predicted = ACTIONS[action_id]\n    expected = scenario['expected']\n    passed = predicted == expected\n    \n    status = \"âœ… PASS\" if passed else \"âŒ FAIL\"\n    results['pass' if passed else 'fail'] += 1\n    action_results[expected]['pass' if passed else 'fail'] += 1\n    \n    print(f\"\\n{scenario['name']} [{scenario['severity'].upper()}]\")\n    print(f\"   Expected: {expected}\")\n    print(f\"   Got:      {predicted} {status}\")\n    print(f\"   Rationale: {scenario['rationale']}\")\n    print(f\"   Q-values:\")\n    for i, (name, q) in enumerate(zip(ACTIONS, q_values)):\n        marker = \" â† CHOSEN\" if i == action_id else \"\"\n        exp_marker = \" (expected)\" if name == expected else \"\"\n        print(f\"      {name:20s}: {q:7.3f}{marker}{exp_marker}\")\n\n# =============================================================================\n# Summary\n# =============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ðŸ“Š TEST SUMMARY\")\nprint(\"=\" * 70)\ntotal = results['pass'] + results['fail']\nprint(f\"\\nOverall: {results['pass']}/{total} passed ({100*results['pass']/total:.1f}%)\")\n\nprint(\"\\nBy Action:\")\nfor action in ACTIONS:\n    p = action_results[action]['pass']\n    f = action_results[action]['fail']\n    total = p + f\n    if total > 0:\n        pct = 100 * p / total\n        print(f\"   {action:20s}: {p}/{total} ({pct:.0f}%)\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"âœ… Test suite complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Deployment Instructions\n",
    "\n",
    "### Copy to Jetson\n",
    "```bash\n",
    "scp kaal_policy.pth user@jetson-ip:~/e-raksha/models/\n",
    "```\n",
    "\n",
    "### Verify on Jetson\n",
    "```python\n",
    "from core.agentic_defender import AgenticDefender\n",
    "import yaml\n",
    "\n",
    "with open('config/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update config to use new model\n",
    "config['agent']['model_path'] = 'models/kaal_policy.pth'\n",
    "\n",
    "agent = AgenticDefender(config)\n",
    "print(f'Model loaded: {agent.model_loaded}')\n",
    "print(f'Mode: {agent.get_statistics()[\"mode\"]}')\n",
    "```\n",
    "\n",
    "### Run RAKSHAK\n",
    "```bash\n",
    "sudo python main.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}