{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›¡ï¸ KAAL Offline RL Training Notebook\n",
    "\n",
    "## RAKSHAK - Agentic AI Cyber Guardian\n",
    "\n",
    "This notebook trains the **KAAL (Knowledge-Augmented Autonomous Learner)** Dueling DQN agent for autonomous cyber defense.\n",
    "\n",
    "### What is KAAL?\n",
    "KAAL is a reinforcement learning agent that decides defensive actions against cyber threats:\n",
    "- **MONITOR** - Continue observing the threat\n",
    "- **DEPLOY_HONEYPOT** - Deploy a decoy to gather intelligence\n",
    "- **ISOLATE_DEVICE** - Quarantine the compromised device\n",
    "- **ENGAGE_ATTACKER** - Redirect attacker to honeypot\n",
    "- **ALERT_USER** - Send notification to user\n",
    "\n",
    "### Training Approach\n",
    "- **Offline RL**: Train from stored attack events (no live environment needed)\n",
    "- **Dueling DQN**: Separates value and advantage for better learning\n",
    "- **Experience Replay**: Random sampling for stable training\n",
    "\n",
    "### Output\n",
    "- `kaal_policy.pth` - Trained model for Jetson deployment\n",
    "\n",
    "---\n",
    "**Author**: Team RAKSHAK  \n",
    "**Runtime**: GPU recommended (T4 or better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy matplotlib tqdm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving model)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/RAKSHAK_Models'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Dueling DQN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling Deep Q-Network architecture.\n",
    "    \n",
    "    Separates the network into value and advantage streams:\n",
    "    Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\n",
    "    \n",
    "    This helps the agent learn which states are valuable\n",
    "    without having to learn the effect of each action.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int = 10, action_size: int = 5, hidden_size: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Shared feature extraction\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Value stream - estimates V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream - estimates A(s, a)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, action_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.feature(x)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        \n",
    "        # Combine: Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    \"\"\"RL transition tuple.\"\"\"\n",
    "    state: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    done: bool\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, transition: Transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size: int) -> List[Transition]:\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Test the architecture\n",
    "model = DuelingDQN().to(device)\n",
    "test_input = torch.randn(1, 10).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"âœ… DuelingDQN created successfully!\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "print(f\"   Output shape: {test_output.shape}\")\n",
    "print(f\"   Q-values: {test_output.detach().cpu().numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Training Data\n",
    "\n",
    "Choose one of the options below:\n",
    "- **Option A**: Generate synthetic training data (for testing)\n",
    "- **Option B**: Upload real events from Jetson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# OPTION A: Generate Synthetic Training Data\n# =============================================================================\n# Use this for testing the training pipeline\n\ndef generate_synthetic_events(num_events: int = 5000) -> List[Dict]:\n    \"\"\"\n    Generate synthetic attack events for training.\n    \n    Uses SEVERITY-BASED optimal action selection to train the model\n    to respond appropriately based on threat severity.\n    \"\"\"\n    ATTACK_TYPES = ['port_scan', 'brute_force', 'dos_attack', 'malware', \n                    'exploit_attempt', 'data_exfiltration', 'unauthorized_access']\n    SEVERITIES = ['low', 'medium', 'high', 'critical']\n    ACTIONS = ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']\n    \n    # SEVERITY-BASED optimal action mapping (KEY FIX!)\n    # Critical/High threats -> ISOLATE immediately\n    # Medium threats -> ENGAGE to gather intel\n    # Low threats -> MONITOR or deploy honeypot\n    OPTIMAL_BY_SEVERITY = {\n        'critical': ['ISOLATE_DEVICE'],\n        'high': ['ISOLATE_DEVICE', 'ENGAGE_ATTACKER'],\n        'medium': ['ENGAGE_ATTACKER', 'DEPLOY_HONEYPOT'],\n        'low': ['MONITOR', 'DEPLOY_HONEYPOT']\n    }\n    \n    events = []\n    base_time = datetime.now()\n    \n    for i in range(num_events):\n        attack_type = random.choice(ATTACK_TYPES)\n        severity = random.choice(SEVERITIES)\n        \n        # Severity affects state vector\n        severity_val = SEVERITIES.index(severity) / 3.0\n        attack_val = ATTACK_TYPES.index(attack_type) / 7.0\n        \n        # Generate realistic state vector\n        state = [\n            attack_val,                           # attack_type\n            severity_val,                         # severity (0=low, 1=critical)\n            random.random(),                      # source_port (normalized)\n            random.random() * 0.2,                # target_port (typically low)\n            random.random() * severity_val,       # packets_per_sec\n            random.random(),                      # duration\n            1.0 if random.random() > 0.7 else 0.0,  # is_known_attacker\n            random.random() * 0.5 + severity_val * 0.5,  # device_risk\n            random.random(),                      # time_of_day\n            random.random() * 0.3 + 0.2          # protocol_risk\n        ]\n        \n        # Choose action based on SEVERITY (not attack type)\n        if random.random() > 0.3:\n            # 70% of time: choose severity-optimal action\n            action = random.choice(OPTIMAL_BY_SEVERITY.get(severity, ACTIONS))\n        else:\n            # 30% exploration: random action\n            action = random.choice(ACTIONS)\n        \n        action_id = ACTIONS.index(action)\n        \n        # Outcome depends on severity-action match\n        optimal = action in OPTIMAL_BY_SEVERITY.get(severity, [])\n        success = optimal or random.random() > 0.5\n        \n        events.append({\n            'event_id': f'evt-{i:05d}',\n            'timestamp': (base_time - timedelta(minutes=i*5)).isoformat(),\n            'source_ip': f'192.168.1.{random.randint(100, 200)}',\n            'target_ip': '192.168.1.1',\n            'attack_type': attack_type,\n            'severity': severity,\n            'state_vector': state,\n            'action_taken': action,\n            'action_id': action_id,\n            'outcome': 'blocked' if action == 'ISOLATE_DEVICE' else 'engaged',\n            'outcome_success': success,\n            'metadata': {'ttp_captured': success and action == 'ENGAGE_ATTACKER'}\n        })\n    \n    return events\n\n# Generate events\nUSE_SYNTHETIC = True  # Set to False if uploading real events\n\nif USE_SYNTHETIC:\n    events = generate_synthetic_events(10000)\n    print(f\"âœ… Generated {len(events)} synthetic events\")\n    print(f\"   Sample event: {events[0]['attack_type']} ({events[0]['severity']}) -> {events[0]['action_taken']}\")\n    \n    # Show severity distribution\n    from collections import Counter\n    sev_counts = Counter([e['severity'] for e in events])\n    print(f\"\\nðŸ“Š Severity Distribution:\")\n    for sev in ['low', 'medium', 'high', 'critical']:\n        print(f\"   {sev}: {sev_counts[sev]} ({100*sev_counts[sev]/len(events):.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTION B: Upload Real Events from Jetson\n",
    "# =============================================================================\n",
    "# Run this cell if you have real events exported from RAKSHAK\n",
    "\n",
    "UPLOAD_EVENTS = False  # Set to True to upload\n",
    "\n",
    "if UPLOAD_EVENTS:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"ðŸ“¤ Upload your events JSON file(s):\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    events = []\n",
    "    for filename, content in uploaded.items():\n",
    "        data = json.loads(content.decode('utf-8'))\n",
    "        if isinstance(data, dict) and 'events' in data:\n",
    "            events.extend(data['events'])\n",
    "        elif isinstance(data, list):\n",
    "            events.extend(data)\n",
    "        print(f\"   Loaded {len(events)} events from {filename}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Total events loaded: {len(events)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Reward Computation & Transition Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_reward(event: Dict) -> float:\n    \"\"\"\n    Compute reward based on SEVERITY-ACTION coupling.\n    \n    Key insight: The optimal action depends on threat SEVERITY, not just attack type.\n    - Critical threats -> ISOLATE immediately (no time to gather intel)\n    - High threats -> ISOLATE or ENGAGE\n    - Medium threats -> Best for intel gathering (ENGAGE/HONEYPOT)\n    - Low threats -> MONITOR to conserve resources\n    \n    This prevents policy collapse toward a single action.\n    \"\"\"\n    severity = event.get('severity', 'medium')\n    action = event.get('action_taken', 'MONITOR')\n    success = event.get('outcome_success', True)\n    \n    # Severity-action optimal mapping (MUST MATCH generate_synthetic_events!)\n    OPTIMAL_BY_SEVERITY = {\n        'critical': ['ISOLATE_DEVICE'],\n        'high': ['ISOLATE_DEVICE', 'ENGAGE_ATTACKER'],\n        'medium': ['ENGAGE_ATTACKER', 'DEPLOY_HONEYPOT'],\n        'low': ['MONITOR', 'DEPLOY_HONEYPOT']\n    }\n    \n    # Base reward scales with severity\n    BASE_REWARDS = {\n        'critical': 10.0,\n        'high': 5.0,\n        'medium': 2.0,\n        'low': 0.5\n    }\n    \n    optimal_actions = OPTIMAL_BY_SEVERITY.get(severity, [])\n    is_optimal = action in optimal_actions\n    base = BASE_REWARDS.get(severity, 1.0)\n    \n    if success:\n        if is_optimal:\n            # High reward for severity-appropriate action\n            reward = base * 2.0\n            \n            # Bonus for TTP capture (only valid for ENGAGE on medium threats)\n            if event.get('metadata', {}).get('ttp_captured'):\n                reward += 2.0\n        else:\n            # Suboptimal action - reduced reward\n            # Penalize more for critical threats handled incorrectly\n            penalty = 0.3 if severity in ['critical', 'high'] else 0.5\n            reward = base * penalty\n    else:\n        # Failed action - negative reward\n        reward = -base\n    \n    return reward\n\n\ndef build_transitions(events: List[Dict]) -> List[Transition]:\n    \"\"\"\n    Convert events to RL transitions.\n    \n    Each event becomes (state, action, reward, next_state, done).\n    \"\"\"\n    transitions = []\n    \n    for i, event in enumerate(events):\n        state = event.get('state_vector', [0.0] * 10)\n        if len(state) != 10:\n            continue\n        \n        state = np.array(state, dtype=np.float32)\n        action = event.get('action_id', 0)\n        reward = compute_reward(event)\n        \n        # Get next state\n        if i + 1 < len(events):\n            next_state = events[i + 1].get('state_vector', [0.0] * 10)\n            done = False\n        else:\n            next_state = [0.0] * 10\n            done = True\n        \n        next_state = np.array(next_state, dtype=np.float32)\n        \n        transitions.append(Transition(\n            state=state,\n            action=action,\n            reward=reward,\n            next_state=next_state,\n            done=done\n        ))\n    \n    return transitions\n\n\n# Build transitions\ntransitions = build_transitions(events)\nprint(f\"âœ… Built {len(transitions)} transitions\")\n\n# Analyze rewards\nrewards = [t.reward for t in transitions]\nprint(f\"\\nðŸ“Š Reward Statistics:\")\nprint(f\"   Mean: {np.mean(rewards):.2f}\")\nprint(f\"   Std:  {np.std(rewards):.2f}\")\nprint(f\"   Min:  {np.min(rewards):.2f}\")\nprint(f\"   Max:  {np.max(rewards):.2f}\")\n\n# Action distribution\nactions = [t.action for t in transitions]\naction_names = ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']\nprint(f\"\\nðŸ“Š Action Distribution:\")\nfor i, name in enumerate(action_names):\n    count = actions.count(i)\n    pct = 100*count/len(actions) if len(actions) > 0 else 0\n    print(f\"   {name}: {count} ({pct:.1f}%)\")\n\n# Show reward by action (should be balanced now!)\nprint(f\"\\nðŸ“Š Average Reward by Action:\")\nfor i, name in enumerate(action_names):\n    action_rewards = [t.reward for t in transitions if t.action == i]\n    if action_rewards:\n        print(f\"   {name}: {np.mean(action_rewards):.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training Configuration\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'epochs': 200,           # Number of training epochs\n",
    "    'batch_size': 64,        # Batch size\n",
    "    'learning_rate': 0.001,  # Learning rate\n",
    "    'gamma': 0.99,           # Discount factor\n",
    "    'target_update': 10,     # Update target network every N epochs\n",
    "    'state_size': 10,        # State vector size\n",
    "    'action_size': 5,        # Number of actions\n",
    "    'hidden_size': 128       # Hidden layer size\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ Training Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kaal(\n",
    "    transitions: List[Transition],\n",
    "    config: Dict,\n",
    "    device: torch.device\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train KAAL Dueling DQN using offline experience replay.\n",
    "    \n",
    "    Returns:\n",
    "        (policy_net, losses, best_loss)\n",
    "    \"\"\"\n",
    "    # Initialize networks\n",
    "    policy_net = DuelingDQN(\n",
    "        config['state_size'],\n",
    "        config['action_size'],\n",
    "        config['hidden_size']\n",
    "    ).to(device)\n",
    "    \n",
    "    target_net = DuelingDQN(\n",
    "        config['state_size'],\n",
    "        config['action_size'],\n",
    "        config['hidden_size']\n",
    "    ).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Load transitions into replay buffer\n",
    "    replay_buffer = ReplayBuffer(capacity=len(transitions) + 1000)\n",
    "    for t in transitions:\n",
    "        replay_buffer.push(t)\n",
    "    \n",
    "    print(f\"\\nðŸš€ Starting Training...\")\n",
    "    print(f\"   Transitions: {len(replay_buffer)}\")\n",
    "    print(f\"   Epochs: {config['epochs']}\")\n",
    "    print(f\"   Batch size: {config['batch_size']}\")\n",
    "    \n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    progress = tqdm(range(config['epochs']), desc=\"Training\")\n",
    "    \n",
    "    for epoch in progress:\n",
    "        epoch_losses = []\n",
    "        steps_per_epoch = max(1, len(transitions) // config['batch_size'])\n",
    "        \n",
    "        for step in range(steps_per_epoch):\n",
    "            # Sample batch\n",
    "            batch = replay_buffer.sample(config['batch_size'])\n",
    "            \n",
    "            # Prepare tensors\n",
    "            states = torch.FloatTensor([t.state for t in batch]).to(device)\n",
    "            actions = torch.LongTensor([t.action for t in batch]).to(device)\n",
    "            rewards = torch.FloatTensor([t.reward for t in batch]).to(device)\n",
    "            next_states = torch.FloatTensor([t.next_state for t in batch]).to(device)\n",
    "            dones = torch.FloatTensor([float(t.done) for t in batch]).to(device)\n",
    "            \n",
    "            # Compute Q(s, a)\n",
    "            current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "            \n",
    "            # Double DQN: select actions with policy, evaluate with target\n",
    "            with torch.no_grad():\n",
    "                next_actions = policy_net(next_states).argmax(1, keepdim=True)\n",
    "                next_q = target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "                target_q = rewards + (1 - dones) * config['gamma'] * next_q\n",
    "            \n",
    "            # Huber loss for stability\n",
    "            loss = F.smooth_l1_loss(current_q.squeeze(), target_q)\n",
    "            \n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        # Epoch stats\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        # Update target network\n",
    "        if (epoch + 1) % config['target_update'] == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        # Track best\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress.set_postfix({'loss': f'{avg_loss:.4f}', 'best': f'{best_loss:.4f}'})\n",
    "    \n",
    "    return policy_net, losses, best_loss\n",
    "\n",
    "\n",
    "# Train!\n",
    "policy_net, losses, best_loss = train_kaal(transitions, CONFIG, device)\n",
    "\n",
    "print(f\"\\nâœ… Training Complete!\")\n",
    "print(f\"   Final Loss: {losses[-1]:.4f}\")\n",
    "print(f\"   Best Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, 'b-', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('KAAL Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot smoothed loss\n",
    "window = 10\n",
    "smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(smoothed, 'g-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Smoothed Loss')\n",
    "plt.title(f'KAAL Training Loss (smoothed, window={window})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Save & Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "checkpoint = {\n",
    "    'policy_state_dict': policy_net.state_dict(),\n",
    "    'state_size': CONFIG['state_size'],\n",
    "    'action_size': CONFIG['action_size'],\n",
    "    'hidden_size': CONFIG['hidden_size'],\n",
    "    'training_info': {\n",
    "        'epochs': CONFIG['epochs'],\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'learning_rate': CONFIG['learning_rate'],\n",
    "        'gamma': CONFIG['gamma'],\n",
    "        'transitions': len(transitions),\n",
    "        'final_loss': losses[-1],\n",
    "        'best_loss': best_loss,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to Google Drive\n",
    "model_path = f\"{OUTPUT_DIR}/kaal_policy.pth\"\n",
    "torch.save(checkpoint, model_path)\n",
    "print(f\"âœ… Model saved to Google Drive: {model_path}\")\n",
    "\n",
    "# Also save locally for download\n",
    "local_path = '/content/kaal_policy.pth'\n",
    "torch.save(checkpoint, local_path)\n",
    "print(f\"âœ… Model saved locally: {local_path}\")\n",
    "\n",
    "# Save inference-only version (smaller)\n",
    "inference_checkpoint = {\n",
    "    'policy_state_dict': policy_net.state_dict(),\n",
    "    'state_size': CONFIG['state_size'],\n",
    "    'action_size': CONFIG['action_size']\n",
    "}\n",
    "inference_path = f\"{OUTPUT_DIR}/kaal_policy_inference.pth\"\n",
    "torch.save(inference_checkpoint, inference_path)\n",
    "print(f\"âœ… Inference model saved: {inference_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ðŸ“¥ Downloading model...\")\n",
    "files.download(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "ACTIONS = ['MONITOR', 'DEPLOY_HONEYPOT', 'ISOLATE_DEVICE', 'ENGAGE_ATTACKER', 'ALERT_USER']\n",
    "\n",
    "def test_model(model, state_vector, device):\n",
    "    \"\"\"Test model on a state vector.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.FloatTensor(state_vector).unsqueeze(0).to(device)\n",
    "        q_values = model(state).cpu().numpy().flatten()\n",
    "        action = np.argmax(q_values)\n",
    "    return action, q_values\n",
    "\n",
    "\n",
    "# Test scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        'name': 'Critical DDoS Attack',\n",
    "        'state': [0.43, 1.0, 0.5, 0.01, 0.9, 0.8, 1.0, 0.9, 0.3, 0.5]\n",
    "    },\n",
    "    {\n",
    "        'name': 'Low Severity Port Scan',\n",
    "        'state': [0.0, 0.0, 0.3, 0.02, 0.1, 0.1, 0.0, 0.2, 0.5, 0.3]\n",
    "    },\n",
    "    {\n",
    "        'name': 'Brute Force Attack',\n",
    "        'state': [0.14, 0.67, 0.8, 0.02, 0.5, 0.3, 0.0, 0.5, 0.1, 0.4]\n",
    "    },\n",
    "    {\n",
    "        'name': 'Malware Detected',\n",
    "        'state': [0.57, 1.0, 0.2, 0.05, 0.3, 0.5, 1.0, 0.8, 0.7, 0.6]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing Model on Sample Threats\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    action, q_values = test_model(policy_net, scenario['state'], device)\n",
    "    print(f\"\\nðŸ“Œ {scenario['name']}\")\n",
    "    print(f\"   Decision: {ACTIONS[action]}\")\n",
    "    print(f\"   Q-values:\")\n",
    "    for i, (name, q) in enumerate(zip(ACTIONS, q_values)):\n",
    "        marker = \"  â†\" if i == action else \"\"\n",
    "        print(f\"      {name:20s}: {q:7.3f}{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Model testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Deployment Instructions\n",
    "\n",
    "### Copy to Jetson\n",
    "```bash\n",
    "scp kaal_policy.pth user@jetson-ip:~/e-raksha/models/\n",
    "```\n",
    "\n",
    "### Verify on Jetson\n",
    "```python\n",
    "from core.agentic_defender import AgenticDefender\n",
    "import yaml\n",
    "\n",
    "with open('config/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update config to use new model\n",
    "config['agent']['model_path'] = 'models/kaal_policy.pth'\n",
    "\n",
    "agent = AgenticDefender(config)\n",
    "print(f'Model loaded: {agent.model_loaded}')\n",
    "print(f'Mode: {agent.get_statistics()[\"mode\"]}')\n",
    "```\n",
    "\n",
    "### Run RAKSHAK\n",
    "```bash\n",
    "sudo python main.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}